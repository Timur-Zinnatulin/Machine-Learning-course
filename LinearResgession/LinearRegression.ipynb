{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "183a7457",
   "metadata": {},
   "source": [
    "# Практическое задание 1: Линейная регрессия, метод градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd64d2ca",
   "metadata": {},
   "source": [
    "## №1 Самостоятельно реализовать функцию gradient_descent(X, y), которая по заданной обучающей выборке обучает модель линейной регрессии, оптимизируя функционал методом градиентного спуска (Batch Gradient Descent, GD) и возвращая вектор весов w. В качестве функционала можно выбрать, например, функцию ошибок MSE + -регуляризатор. Использовать матрично-векторные операции для вычисления градиента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b633ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Убираем предупреждения FutureWarning\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def gradient_descent(X, y, alpha = 0.01, iterations = 1000):\n",
    "    w = np.zeros(X.shape[1])\n",
    "    n = X.shape[0]\n",
    "    for i in range(iterations):\n",
    "        y_pred = np.dot(X, w)\n",
    "        for j in range(X.shape[1]):\n",
    "            w[j] -= alpha * (1/n * 2 * np.sum(X[:, j] * (y_pred - y)))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3042ee95",
   "metadata": {},
   "source": [
    "## №2 Найти данные, на которых интересно будет решать задачу регрессии. Зависимость целового признака от нецелевых должна быть не слишком сложной, чтобы обученная линейная модель смогла показать приемлимый результат."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cb11f0",
   "metadata": {},
   "source": [
    "Выбран датасет red-wine-quality.csv(https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009), в котором содержится информация о качестве красного вина.\n",
    "\n",
    "Первые 11 признаков - химические свойства вина, а 12 - оценка качества вина по шкале от 0 до 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883434e2",
   "metadata": {},
   "source": [
    "## №3 Считать данные, выполнить первичный анализ данных, при необходимости произвести чистку данных (Data Cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ce153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('red-wine-quality.csv', sep=',')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2179c1c",
   "metadata": {},
   "source": [
    "Данные:\n",
    "1. fixed acidity - фиксированная кислотность\n",
    "2. volatile acidity - летучая кислотность\n",
    "3. citric acid - лимонная кислота\n",
    "4. residual sugar - остаточный сахар\n",
    "5. chlorides - хлориды\n",
    "6. free sulfur dioxide - свободный диоксид серы\n",
    "7. total sulfur dioxide - общий диоксид серы\n",
    "8. density - плотность\n",
    "9. pH - pH\n",
    "10. sulphates - сульфаты\n",
    "11. alcohol - алкоголь\n",
    "12. quality - качество (целевой признак)\n",
    "\n",
    "Все признаки являются количественными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45981a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1b5244",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91cca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Количество пропусков в данных'''\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5277e7be",
   "metadata": {},
   "source": [
    "Пропусков в данных нет"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229453cc",
   "metadata": {},
   "source": [
    "## №4 Выполнить разведочный анализ (EDA), использовать визуализацию, сделать выводы, которые могут быть полезны при дальнейшем решении задачи регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed5ac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "'''Корреляция признаков'''\n",
    "corr = data.corr()\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(corr, vmax=1, square=True, annot=True, cmap='RdYlGn')\n",
    "plt.title('Correlation between different wine fearures')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e824723",
   "metadata": {},
   "source": [
    "Выводы:\n",
    "- Сильную корреляцию с целевым признаком имеют признаки: alcohol, volatile acidity, citric acid, sulphates\n",
    "- Слабую корреляцию с целевым признаком имеют признаки: fixed acidity, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68b90e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "sns.countplot(x='quality', data=data)\n",
    "plt.title('Count of quality ranks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acab5a6d",
   "metadata": {},
   "source": [
    "- Больше всего вина с уровнем качества 5 и 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac39eca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "sns.boxplot(x='quality', y='alcohol', data=data)\n",
    "plt.title('Alcohol content in wine with relation to quality')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922570da",
   "metadata": {},
   "source": [
    "Выводы:\n",
    "- Алкоголь в вине влияет на его качество: чем больше содержание алкоголя в вине, тем выше его качество. \n",
    "- Есть выбросы в данных, но они не сильно влияют на итоговый результат."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27db07ec",
   "metadata": {},
   "source": [
    "## №5 При необходимости выполнить полезные преобразования данных (например, трансформировать категариальные признаки в количественные), убрать ненужные признаки, создать новые (Feature Engineering)\n",
    "Необходимости не возникло :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d2eabf",
   "metadata": {},
   "source": [
    "##  Дополнительное задание. Перед обучением моделей подобрать наилучшее количество (и само подмножество) признаков, например используя Recursive Feature Elimination (RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41892787",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''RFE'''\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "X = data.drop(['quality'], axis=1)\n",
    "y = data['quality']\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "rfe = RFE(model, n_features_to_select=6)\n",
    "fit = rfe.fit(X, y)\n",
    "\n",
    "for (i, j) in zip(X.columns, fit.support_):\n",
    "    if j != True:\n",
    "        X = X.drop([i], axis=1)\n",
    "\n",
    "list(X.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0af3d5",
   "metadata": {},
   "source": [
    "## №6 Случайным образом разбить данные на обучающую и тестовую выборки, используя методы существующих библиотек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311a95d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data.drop(['quality'], axis=1)\n",
    "y = data['quality']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4e44cd",
   "metadata": {},
   "source": [
    "## №7 При обучении моделей использовать масштабирование данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55c9e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeb2019",
   "metadata": {},
   "source": [
    "## №8 Обучить модель на обучающей выборке, используя функцию gradient_descent(X, y). Оценить качество модели на обучающей и тестовой выборках, используя MSE, RMSE и R2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9c7ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = np.hstack([np.ones((X_train_scaled.shape[0], 1)), X_train_scaled])\n",
    "X_test_scaled = np.hstack([np.ones((X_test_scaled.shape[0], 1)), X_test_scaled])\n",
    "w = gradient_descent(X_train_scaled, y_train)\n",
    "y_pred = X_test_scaled.dot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89534a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Display Mse, RMSE and R2 metrics'''\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "my_gradient_descent_mse = mean_squared_error(y_test, y_pred)\n",
    "my_gradient_descent_rmse = np.sqrt(my_gradient_descent_mse)\n",
    "my_gradient_descent_r2 = r2_score(y_test, y_pred)\n",
    "print('MSE: ', my_gradient_descent_mse)\n",
    "print('RMSE: ', my_gradient_descent_rmse)\n",
    "print('R2: ', my_gradient_descent_r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9258aa",
   "metadata": {},
   "source": [
    "## №9 Обучить модель, используя существующую библиотеку. Например, в sklearn для -регуляризатора можно использовать Ridge. Сравнить качество с вашей реализацией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a56d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "model = Ridge()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd26051",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Display Mse, RMSE and R2 metrics'''\n",
    "Ridge_mse = mean_squared_error(y_test, y_pred)\n",
    "Ridge_rmse = np.sqrt(Ridge_mse)\n",
    "Ridge_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print('MSE: ', Ridge_mse)\n",
    "print('RMSE: ', Ridge_rmse)\n",
    "print('R2: ', Ridge_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c8288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "table = PrettyTable()\n",
    "table.field_names = ['Model', 'MSE', 'RMSE', 'R2']\n",
    "table.add_row(['Ridge', Ridge_mse, Ridge_rmse, Ridge_r2])\n",
    "table.add_row(['Gradient_descent', my_gradient_descent_mse, my_gradient_descent_rmse, my_gradient_descent_r2])\n",
    "\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ea8f95",
   "metadata": {},
   "source": [
    "Выводы: самостоятельная и библиотечная реализации дают очень схожие результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072773e2",
   "metadata": {},
   "source": [
    "## №10 Повторить тоже самое, но используя кросс-валидацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd23824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print('MSE: ', scores.mean())\n",
    "print('RMSE: ', np.sqrt(-scores.mean()))\n",
    "print('R2: ', r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa3b545",
   "metadata": {},
   "source": [
    "## №11 Создать таблицу, со строками (mse-train, mse-test, rmse-train, rmse-test, r2-train, r2-test) и столбцами (Fold1, Fold2, ..., Foldk, E, STD), где k --- количество фолдов в кросс-валидации, E --- мат. ожидание и STD --- стандартное отклонение. Сделать выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f5e9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "rdg_model = Ridge()\n",
    "scoring = {'neg_mse': 'neg_mean_squared_error',\n",
    "           'neg_rmse': 'neg_root_mean_squared_error',\n",
    "           'r2': 'r2'}\n",
    "\n",
    "kf = KFold(n_splits = 5, random_state=42, shuffle=True)\n",
    "scores = cross_validate(rdg_model, X, y, cv = kf,\n",
    "                                scoring=scoring,\n",
    "                                return_train_score = True)\n",
    "\n",
    "summary_table = pd.DataFrame(scores)\n",
    "\n",
    "summary_table.drop(['fit_time', 'score_time'], axis = 1, inplace = True)\n",
    "summary_table['test_mse'] = -1*summary_table['test_neg_mse']\n",
    "summary_table['train_mse'] = -1*summary_table['train_neg_mse']\n",
    "summary_table['test_rmse'] = -1*summary_table['test_neg_rmse']\n",
    "summary_table['train_rmse'] = -1*summary_table['train_neg_rmse']\n",
    "summary_table.drop(['test_neg_mse', 'train_neg_mse', 'test_neg_rmse', 'train_neg_rmse'], axis = 1, inplace = True)\n",
    "summary_table = summary_table.transpose()\n",
    "summary_table.set_axis(['Fold2','Fold3','Fold4','Fold5','Fold6'], axis=1, copy=False)\n",
    "summary_table['E'] = summary_table.mean(axis = 1)\n",
    "summary_table['STD'] = summary_table.std(axis = 1)\n",
    "\n",
    "summary_table.style.format(\"{:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdff943",
   "metadata": {},
   "source": [
    "Выводы:\n",
    "- Модель с кросс-валидацией работает лучше, чем без нее\n",
    "- Метрики качества модели на тестовой выборке ниже, чем на обучающей, что говорит о том, что модель не переобучена\n",
    "- При увеличении количества фолдов качество модели улучшается\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69748849",
   "metadata": {},
   "source": [
    "## Доп. задание 2. Также самостоятельно реализовать метод стохастического градиентного спуска (Stochastic Gradient Descent, SGD), обучить модели и добавить их во все сравнения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106195f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''stochastic gradient descent'''\n",
    "def stochastic_gradient_descent(X, y, w, eta=0.01, n_iterations=100):\n",
    "    m = X.shape[0]\n",
    "    for iteration in range(n_iterations):\n",
    "        for i in range(m):\n",
    "            random_index = np.random.randint(m)\n",
    "            xi = X[random_index:random_index+1]\n",
    "            yi = y[random_index:random_index+1]\n",
    "            gradients = 2/m * xi.T.dot(xi.dot(w) - yi)\n",
    "            w = w - eta * gradients\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d359bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.zeros(X_train_scaled.shape[1])\n",
    "w = stochastic_gradient_descent(X_train_scaled, y_train, w)\n",
    "y_pred = X_test_scaled.dot(w)\n",
    "\n",
    "''' Display Mse, RMSE and R2 metrics'''\n",
    "my_stochastic_gradient_descent_mse = mean_squared_error(y_test, y_pred)\n",
    "my_stochastic_gradient_descent_rmse = np.sqrt(my_stochastic_gradient_descent_mse)\n",
    "my_stochastic_gradient_descent_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print('MSE: ', my_stochastic_gradient_descent_mse)\n",
    "print('RMSE: ', my_stochastic_gradient_descent_rmse)\n",
    "print('R2: ', my_stochastic_gradient_descent_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac648329",
   "metadata": {},
   "source": [
    "## Доп. задание 3. Также самостоятельно реализовать метод мини-пакетного градиентного спуска (Mini Batch Gradient Descent), обучить модели и добавить их во все сравнения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9c6ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_gradient_descent(X, y, batch_size=20, alpha=0.01, n_iterations=100):\n",
    "    w = np.zeros(X.shape[1])\n",
    "    m = X.shape[0]\n",
    "    for i in range(n_iterations):\n",
    "        for j in range(0, m, batch_size):\n",
    "            X_batch = X[j:j+batch_size]\n",
    "            y_batch = y[j:j+batch_size]\n",
    "            w = w - alpha * (1/batch_size) * np.dot(X_batch.T, (np.dot(X_batch, w) - y_batch))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116214bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = mini_batch_gradient_descent(X_train_scaled, y_train)\n",
    "y_pred = X_test_scaled.dot(w)\n",
    "\n",
    "''' Display Mse, RMSE and R2 metrics'''\n",
    "my_mini_batch_gradient_descent_mse = mean_squared_error(y_test, y_pred)\n",
    "my_mini_batch_gradient_descent_rmse = np.sqrt(my_mini_batch_gradient_descent_mse)\n",
    "my_mini_batch_gradient_descent_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print('MSE: ', my_mini_batch_gradient_descent_mse)\n",
    "print('RMSE: ', my_mini_batch_gradient_descent_rmse)\n",
    "print('R2: ', my_mini_batch_gradient_descent_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295f3b15",
   "metadata": {},
   "source": [
    "### Свобдная таблица с результатами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea837ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "x = PrettyTable()\n",
    "x.field_names = [\"Model\", \"MSE\", \"RMSE\", \"R2\"]\n",
    "x.add_row([\"My Linear Regression\", my_gradient_descent_mse, my_gradient_descent_rmse, my_gradient_descent_r2])\n",
    "x.add_row([\"Ridge Regression\", Ridge_mse, Ridge_rmse, Ridge_r2])\n",
    "x.add_row([\"My Stochastic Gradient Descent\", my_stochastic_gradient_descent_mse, my_stochastic_gradient_descent_rmse, my_stochastic_gradient_descent_r2])\n",
    "x.add_row([\"My Mini Batch Gradient Descent\", my_mini_batch_gradient_descent_mse, my_mini_batch_gradient_descent_rmse, my_mini_batch_gradient_descent_r2])\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "ddaee25c3d36155007e8d8878de890371d06e49958582f58a3b72c62fc780c83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
