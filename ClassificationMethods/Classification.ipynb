{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача 2. Сравнение методов классификации\n",
    "\n",
    "### Выполнил Зиннатулин Тимур Раифович, группа 22.М04\n",
    "\n",
    "Выполнены все задачи:\n",
    "- Самостоятельно реализовать один из методов классификации, с возможностью настройки гиперпараметров.\n",
    "- Взять данные для предсказания заболеваний сердца тут (целевой признак для предсказания --- target). Демо блакнот с анализом этих данных можно найти тут.\n",
    "- Считать данные, выполнить первичный анализ данных, при необходимости произвести чистку данных (Data Cleaning).\n",
    "- Выполнить разведочный анализ (EDA), использовать визуализацию, сделать выводы, которые могут быть полезны при дальнейшем решении задачи классификации.\n",
    "- При необходимости выполнить полезные преобразования данных (например, трансформировать категариальные признаки в количественные), убрать ненужные признаки, создать новые (Feature Engineering).\n",
    "- Используя подбор гиперпараметров, кросс-валидацию и при необходимости масштабирование данных, добиться наилучшего качества предсказания от Вашей реализации на выделенной заранее тестовой выборке.\n",
    "- Повторить предыдущий пункт для библиотечных реализаций (например, из sklearn) всех пройденных методов классификации (logistic regression, svm, knn, naive bayes, decision tree).\n",
    "- Сравнить все обученные модели, построить их confusion matrices. Сделать выводы о полученных моделях в рамках решения задачи классификации на выбранных данных.\n",
    "* (+2 балла) Реализовать еще один из методов классификации и добавить его в сравнение.\n",
    "* (+2 балла) Найти данные, на которых интересно будет решать задачу классификации. Повторить все пункты задания на новых данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Самостоятельно реализовать один из методов классификации, с возможностью настройки гиперпараметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "'''\n",
    "Метод логистической регрессии\n",
    "'''\n",
    "def logistic_regression(X, y, w, alpha, n_iters):\n",
    "    '''\n",
    "    Входные параметры:\n",
    "    X - матрица объекты-признаки\n",
    "    y - вектор правильных ответов\n",
    "    alpha - шаг градиентного спуска\n",
    "    num_iters - количество итераций градиентного спуска\n",
    "    \n",
    "    Возвращаемые значения:\n",
    "    w - вектор весов\n",
    "    '''\n",
    "\n",
    "    '''Инициализация параметров'''\n",
    "    m = y.size\n",
    "    for i in range(n_iters):\n",
    "        h = sigmoid(X.dot(w))\n",
    "        J = (-1 / m) * (y.T.dot(np.log(h)) + (1 - y).T.dot(np.log(1 - h)))\n",
    "        grad = (1 / m) * X.T.dot(h - y)\n",
    "        w = w - alpha * grad\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Взять данные для предсказания заболеваний сердца [тут](https://github.com/rustam-azimov/ml-course/tree/main/data/heart_disease) (целевой признак для предсказания --- target). Демо блокнот с анализом этих данных можно найти [тут](https://github.com/rustam-azimov/ml-course/blob/main/practice/practice07_knn_nb/practice07_part02_classification_heart_disease_demo.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "data = pd.read_csv('heart.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Считать данные, выполнить первичный анализ данных, при необходимости произвести чистку данных (Data Cleaning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описание данных:\n",
    "1. sex -- пол (1 = мужской, 0 = женский)\n",
    "2. cp -- тип боли в груди (4 значения)\n",
    "3. trestbps -- артериальное давление в состоянии покоя (в мм рт. ст. при поступлении в больницу)\n",
    "4. chol -- уровень холестерина в сыворотке (мг/дл)\n",
    "5. fbs -- уровень сахара в крови натощак > 120 мг/дл (1 = true; 0 = false)\n",
    "6. restecg -- результаты электрокардиографии в состоянии покоя (значения 0,1,2)\n",
    "7. thalach -- достигнутая максимальная частота сердечных сокращений\n",
    "8. exang -- стенокардия, вызванная физической нагрузкой (1 = да; 0 = нет)\n",
    "9. oldpeak -- депрессия ST, вызванная физической нагрузкой по сравнению с отдыхом\n",
    "10. slope -- наклон пикового упражнения ST сегмента (значения 1,2,3)\n",
    "11. ca -- количество крупных сосудов (0-3), окрашенных флуороскопией\n",
    "12. thal -- 3 = нормальный; 6 = исправленный дефект; 7 = обратимый дефект\n",
    "13. target -- наличие или отсутствие сердечного заболевания (0 = отсутствие, 1 = наличие)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чистка данных не нужна, так как:\n",
    "- В данных нет пропусков\n",
    "- В данных нет дубликатов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Выполнить разведочный анализ (EDA), использовать визуализацию, сделать выводы, которые могут быть полезны при дальнейшем решении задачи классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='target', data=data, palette='hls')\n",
    "''' Подписи к осям'''\n",
    "plt.xlabel('Наличие заболевания')\n",
    "plt.ylabel('Количество людей')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По графику целевой переменной видно, что людей с заболеванием сердца больше, чем без него\n",
    "- Это может быть полезно при дальнейшем решении задачи классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим график зависимости целевой переменной от признака \"пол\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('target').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Заболевание сердца чаще встречается у мужчин, чем у женщин"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(data.age, data.target).plot(kind=\"bar\", figsize=(20, 6), color=['green','orange'])\n",
    "plt.title('Зависимость целевой переменной от возраста')\n",
    "plt.xlabel('Возраст')\n",
    "plt.ylabel('Частота')\n",
    "plt.legend([\"Нет заболевания\", \"Есть заболевание\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Из графика видно, что у людей от 40 до 60 лет чаще всего встречается заболевание сердца\n",
    "- А после 60 лет заболевание сердца становится реже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''График зависимости целевой переменной от артериального давления'''\n",
    "pd.crosstab(data.thalach, data.target).plot(kind=\"bar\", figsize=(20, 6), color=['green','orange'])\n",
    "plt.title('Зависимость целевой переменной от артериального давления')\n",
    "plt.xlabel('Артериальное давление в состоянии покоя')\n",
    "plt.ylabel('Частота')\n",
    "plt.legend([\"Нет заболевания\", \"Есть заболевание\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Из графика зависимости целевой переменной от артериального давления видно, что чем выше артериальное давление, тем чаще встречается заболевание сердца"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''График зависимости целевой переменной от chain pain'''\n",
    "pd.crosstab(data.cp, data.target).plot(kind=\"bar\", figsize=(20, 6), color=['green','orange'])\n",
    "plt.title('Зависимость целевой переменной от chain pain')\n",
    "plt.xlabel('Тип боли в груди')\n",
    "plt.ylabel('Частота')\n",
    "plt.legend([\"Нет заболевания\", \"Есть заболевание\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Можно сделать вывод, что чаще всего встречается заболевание у людей с типом боли в груди 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы по разведочному анализу:\n",
    "- Заболевание сердца чаще встречается у мужчин, чем у женщин\n",
    "- Чаще всего встречается заболевание у людей с типом боли в груди 2\n",
    "- Чаще всего встречается заболевание у людей с артериальным давлением 150-180\n",
    "- Чаще всего встречается заболевание у людей от 40 до 60 лет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. При необходимости выполнить полезные преобразования данных (например, трансформировать категариальные признаки в количественные), убрать ненужные признаки, создать новые (Feature Engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['target'], axis=1)\n",
    "y = data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим ненужные признаки, выполнив RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "rfe = RFE(logreg, n_features_to_select=6)\n",
    "rfe = rfe.fit(X, y)\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)\n",
    "\n",
    "'''Печать выбранных признаков'''\n",
    "print(X.columns[rfe.support_])\n",
    "\n",
    "'''Удаление ненужных признаков'''\n",
    "X = X.drop(['trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'oldpeak'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Используя подбор гиперпараметров, кросс-валидацию и при необходимости масштабирование данных, добиться наилучшего качества предсказания от Вашей реализации на выделенной заранее тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Разделение данных на обучающую и тестовую выборки'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Подбор гиперпараметров для нашей реализации logistic_regression alpha and n_inter'''\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "alphas = [0.000001, 0.00001, 0.0001, 0.001]\n",
    "n_iters = [100, 1000, 10000]\n",
    "\n",
    "best_alpha = 0\n",
    "best_n_iter = 0\n",
    "best_score = 0\n",
    "\n",
    "for alpha in alphas:\n",
    "    for n_iter in n_iters:\n",
    "        w = np.zeros(X_train.shape[1])\n",
    "        w = logistic_regression(X_train, y_train, w, alpha, n_iter)\n",
    "        y_pred = sigmoid(np.dot(X_test, w))\n",
    "        y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "        currScore = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        if  currScore > best_score:\n",
    "            best_score = currScore\n",
    "            best_alpha = alpha\n",
    "            best_n_iter = n_iter\n",
    "\n",
    "print(f'best alpha: {best_alpha}, best n_iter: {best_n_iter}, best accuracy: {best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogisticRegression:\n",
    "    def __init__(self, alpha=0.0001, n_iter=100):\n",
    "        self.alpha = alpha\n",
    "        self.n_iter = n_iter\n",
    "        self.w = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        w = np.zeros(X.shape[1])\n",
    "        self.w = logistic_regression(X, y, w, self.alpha, self.n_iter)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = sigmoid(np.dot(X, self.w))\n",
    "        y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "        return y_pred\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        return {'alpha': self.alpha, 'n_iter': self.n_iter}\n",
    "    \n",
    "    def set_params(self, alpha, n_iter):\n",
    "        self.alpha = alpha\n",
    "        self.n_iter = n_iter\n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Кросс валидация для нашей реализации logistic_regression'''\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "scores = []\n",
    "alpha = best_alpha\n",
    "n_iter = best_n_iter\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = MyLogisticRegression(alpha=alpha, n_iter=n_iter)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    scores.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(f'accuracy: {np.mean(scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Масшатбирование данных'''\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = MyLogisticRegression(alpha=best_alpha, n_iter=best_n_iter)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred_my_lr = model.predict(X_test_scaled)\n",
    "\n",
    "print(f'accuracy: {accuracy_score(y_test, y_pred_my_lr)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалось достичь результата 0.7902 на тестовой выборке с масштабированными данными."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Повторить предыдущий пункт для библиотечных реализаций (например, из sklearn) всех пройденных методов классификации (logistic regression, svm, knn, naive bayes, decision tree)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''LogisticRegression из sklearn'''\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "print(f'accuracy: {accuracy_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Подбор гиперпараметров для LogisticRegression из sklearn'''\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000], 'penalty': ['l1', 'l2']}\n",
    "model = LogisticRegression()\n",
    "grid = GridSearchCV(model, params)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "print(grid.best_params_)\n",
    "y_pred_sklearn_lr = grid.predict(X_test_scaled)\n",
    "print(f'accuracy: {accuracy_score(y_test, y_pred_sklearn_lr)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''kross validation для LogisticRegression из sklearn'''\n",
    "from sklearn.model_selection import cross_val_score\n",
    "model = LogisticRegression(C=0.1, penalty='l2')\n",
    "scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "print(f'accuracy: {np.mean(scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''SVM из sklearn'''\n",
    "from sklearn.svm import SVC\n",
    "model = SVC()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "print(f'accuracy: {accuracy_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Подбор гиперпараметров для SVM из sklearn'''\n",
    "params = {'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000], 'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}\n",
    "model = SVC()\n",
    "grid = GridSearchCV(model, params)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "print(grid.best_params_)\n",
    "y_pred_sklearn_svm = grid.predict(X_test_scaled)\n",
    "print(f'accuracy: {accuracy_score(y_test, y_pred_sklearn_svm)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''kross validation для SVM из sklearn'''\n",
    "model = SVC(C=0.1, kernel='rbf')\n",
    "scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "print(f'accuracy: {np.mean(scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''KNN из sklearn'''\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "print(f'accuracy: {accuracy_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Подбор гиперпараметров для KNN из sklearn'''\n",
    "params = {'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'weights': ['uniform', 'distance']}\n",
    "model = KNeighborsClassifier()\n",
    "grid = GridSearchCV(model, params)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "print(grid.best_params_)\n",
    "y_pred_sklearn_knn = grid.predict(X_test_scaled)\n",
    "print(f'accuracy: {accuracy_score(y_test, y_pred_sklearn_knn)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''kross validation для KNN из sklearn'''\n",
    "model = KNeighborsClassifier(n_neighbors=5, weights='uniform')\n",
    "scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "print(f'accuracy: {np.mean(scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Naive Bayes из sklearn'''\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "print(f'accuracy: {accuracy_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Подбор гиперпараметров для Naive Bayes из sklearn'''\n",
    "params = {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1000]}\n",
    "model = GaussianNB()\n",
    "grid = GridSearchCV(model, params)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "print(grid.best_params_)\n",
    "y_pred_sklearn_bayes = grid.predict(X_test_scaled)\n",
    "print(f'accuracy: {accuracy_score(y_test, y_pred_sklearn_bayes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''kross validation для Naive Bayes из sklearn'''\n",
    "model = GaussianNB(var_smoothing=1e-09)\n",
    "scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "print(f'accuracy: {np.mean(scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Decision Tree из sklearn'''\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "print(f'accuracy: {accuracy_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Подбор гиперпараметров для Decision Tree из sklearn'''\n",
    "params = {'criterion': ['gini', 'entropy'], 'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
    "model = DecisionTreeClassifier()\n",
    "grid = GridSearchCV(model, params)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "print(grid.best_params_)\n",
    "y_pred_sklearn_tree = grid.predict(X_test_scaled)\n",
    "print(f'accuracy: {accuracy_score(y_test, y_pred_sklearn_tree)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''kross validation для Decision Tree из sklearn'''\n",
    "model = DecisionTreeClassifier(criterion='gini', max_depth=5)\n",
    "scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "print(f'accuracy: {np.mean(scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Сравнить все обученные модели, построить их confusion matrices. Сделать выводы о полученных моделях в рамках решения задачи классификации на выбранных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Confusuin Matrix для всех рассмотренных методов'''\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "plt.figure(figsize=(24, 12))\n",
    "\n",
    "plt.suptitle(\"Confusion Matrixes\", fontsize=24)\n",
    "plt.subplots_adjust(wspace = 0.6, hspace= 0.4)\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.title(\"SVM\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_sklearn_svm), cmap=\"Blues\", annot=True, fmt=\"d\")\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.title(\"KNN\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_sklearn_knn), cmap='Blues', annot=True, fmt='d')\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.title(\"Naive Bayes\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_sklearn_bayes), cmap='Blues', annot=True, fmt='d')\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.title(\"Decision Tree\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_sklearn_tree), cmap='Blues', annot=True, fmt='d')\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.title(\"Logistic Regression\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_sklearn_lr), cmap='Blues', annot=True, fmt='d')\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.title(\"My logistic regression\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_my_lr), cmap='Blues', annot=True, fmt='d')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы о полученных моделях:\n",
    "- Лучше всего себя показали модели из sklearn: SVM, KNN и Decision Tree\n",
    "- Моя реализация logistic regression показала результат немного хуже, чем реализация из sklearn, однако лучше, чем Naive Bayes из sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительное задание 1 (+2 балла). Реализовать еще один из методов классификации и добавить его в сравнение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''My implementation of SVM for classification'''\n",
    "def svm(X, y, num_iters, lr, verbose=True):\n",
    "    '''\n",
    "    Входные данные:\n",
    "        X - матрица объекты-признаки\n",
    "        y - вектор ответов\n",
    "        num_iters - число итераций градиентного спуска\n",
    "        lr - learning rate\n",
    "    Выходные данные:\n",
    "        w - вектор весов\n",
    "        b - смещение\n",
    "    '''\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.zeros(n_features)\n",
    "    b = 0\n",
    "    y_ = np.where(y <= 0, -1, 1)\n",
    "    for i in range(1, num_iters + 1):\n",
    "        for idx, x_i in enumerate(X):\n",
    "            condition = y_[idx] * (np.dot(x_i, w) - b) >= 1\n",
    "            if condition:\n",
    "                w -= lr * (2 * 1 / i * w)\n",
    "            else:\n",
    "                w -= lr * (2 * 1 / i * w - np.dot(x_i, y_[idx]))\n",
    "                b -= lr * y_[idx]\n",
    "        if verbose and i % 100 == 0:\n",
    "            print(f'iteration: {i}, num_iters: {num_iters}, lr: {lr}')\n",
    "    return w, b                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Class for my implementation of SVM for classification'''\n",
    "class MySVM:\n",
    "    def __init__(self, num_iters=1000, lr=0.001, verbose=False):\n",
    "        self.num_iters = num_iters\n",
    "        self.lr = lr\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.w, self.b = svm(X, y, self.num_iters, self.lr, self.verbose)\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.w) - self.b\n",
    "        labels = np.sign(linear_output)\n",
    "        return np.where(labels <= -1, 0, 1)\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        return {'num_iters': self.num_iters, 'lr': self.lr}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Подпор гиперпараметров для MySVM'''\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'num_iters': [100, 1000, 2000], 'lr': [0.001, 0.01, 0.1]}\n",
    "model = MySVM()\n",
    "grid = GridSearchCV(model, params, scoring='accuracy')\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "print(grid.best_params_)\n",
    "y_pred_my_svm = grid.predict(X_test_scaled)\n",
    "print(f'accuracy: {accuracy_score(y_test, y_pred_my_svm)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Test for MySVM'''\n",
    "\n",
    "model = MySVM(num_iters=100, lr=0.001)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred_my_svm = model.predict(X_test_scaled)\n",
    "print(f'accuracy: {accuracy_score(y_test, y_pred_my_svm)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 12))\n",
    "\n",
    "plt.suptitle(\"Confusion Matrixes\", fontsize=24)\n",
    "plt.subplots_adjust(wspace = 0.6, hspace= 0.4)\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.title(\"SVM\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_sklearn_svm), cmap=\"Blues\", annot=True, fmt=\"d\")\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.title(\"KNN\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_sklearn_knn), cmap='Blues', annot=True, fmt='d')\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.title(\"Naive Bayes\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_sklearn_bayes), cmap='Blues', annot=True, fmt='d')\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.title(\"Decision Tree\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_sklearn_tree), cmap='Blues', annot=True, fmt='d')\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.title(\"Logistic Regression\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_sklearn_lr), cmap='Blues', annot=True, fmt='d')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.suptitle(\"Confusion Matrixes\", fontsize=24)\n",
    "plt.subplots_adjust(wspace = 0.6, hspace= 0.4)\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.title(\"My logistic regression\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_my_lr), cmap='Blues', annot=True, fmt='d')\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.title(\"My SVM\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_my_svm), cmap='Blues', annot=True, fmt='d')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Моя реализация SVM показала результат немного хуже, чем реализация из sklearn\n",
    "- Однако она показала результат лучше, чем реализация из sklearn Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительное задание 2 (+2 балла). Найти данные, на которых интересно будет решать задачу классификации. Повторить все пункты задания на новых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv('gender_classification.csv')\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описание данных:\n",
    "- Всего 891 строк\n",
    "- 12 признаков\n",
    "\n",
    "1. Favorite color -- любимый цыет, категориальный признак\n",
    "2. Favorite music genre -- любимый музыкальный жанр, категориальный признак\n",
    "3. Favorite Deverage -- любимый алкогольный напиток напиток, категориальный признак\n",
    "4. Favorite sott drink -- любимый безалкогольный напиток, категориальный признак\n",
    "5. Gender -- пол, категориальный признак (целевой)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данных нет пропусков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала переведем все признаки в числовые значения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['Favorite Color'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['Favorite Music Genre'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['Favorite Beverage'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['Favorite Soft Drink'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['Favorite Color'] = new_data['Favorite Color'].map({'Cool': 0, 'Warm': 1, 'Neutral': 2})\n",
    "new_data['Favorite Music Genre'] = new_data['Favorite Music Genre'].map({'Rock': 0, 'Hip hop': 1, 'Folk/Traditional': 2, 'Jazz/Blues': 3, 'Pop': 4, 'Electronic': 5, 'R&B and soul': 6})\n",
    "new_data['Favorite Beverage'] = new_data['Favorite Beverage'].map({'Vodka': 0, 'Wine': 1, 'Whiskey': 2, 'Doesn\\'t drink': 3, 'Beer': 4, 'Other': 5})\n",
    "new_data['Favorite Soft Drink'] = new_data['Favorite Soft Drink'].map({'7UP/Sprite': 0, 'Coca Cola/Pepsi': 1, 'Fanta': 2, 'Other': 3})\n",
    "new_data['Gender'] = new_data['Gender'].map({'M':0, 'F':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим график зависимости признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(new_data.corr(), annot=True, cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы:\n",
    "- Сильнее всего коррелируют признаки Favorite Music Genre и Favorite Beverage\n",
    "- Слабо коррелируют признаки Favorite Color и Favorite Soft Drink, а также Gender и Favorite Soft Drink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''countplot gender and favorite color'''\n",
    "plt.figure(figsize=(12, 6))\n",
    "'''Подписи осей'''\n",
    "sns.countplot(x='Favorite Color', hue='Gender', data=new_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы:\n",
    "- Цвета 1 (теплые) больше предпочитают мужчины\n",
    "- Цвета 2 (холодные) больше предпочитают женщины\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_data.drop(['Gender'], axis=1)\n",
    "y = new_data['Gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''RFE'''\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, n_features_to_select=3)\n",
    "fit = rfe.fit(X, y)\n",
    "print(f'Num Features: {fit.n_features_}')\n",
    "print(f'Selected Features: {fit.support_}')\n",
    "\n",
    "X = X.drop(['Favorite Soft Drink'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Logistic Regression'''\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred_lg = model.predict(X_test_scaled)\n",
    "new_data_lg = accuracy_score(y_test, y_pred_lg)\n",
    "print(f'accuracy: {new_data_lg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''GreidSearchCV and cross validation for Logistic Regression'''\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "model = LogisticRegression()\n",
    "grid = {'C': np.logspace(-3, 3, 7), 'penalty': ['l1', 'l2']}\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy', error_score=0)\n",
    "grid_result = grid_search.fit(X_train_scaled, y_train)\n",
    "print(f'Best Score: {grid_result.best_score_} using {grid_result.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''SVM'''\n",
    "model = SVC()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred_svm = model.predict(X_test_scaled)\n",
    "new_data_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f'accuracy: {new_data_svm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''GreidSearchCV and cross validation for SVM'''\n",
    "model = SVC()\n",
    "grid = {'C': np.logspace(-3, 3, 7), 'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy', error_score=0)\n",
    "grid_result = grid_search.fit(X_train_scaled, y_train)\n",
    "print(f'Best Score: {grid_result.best_score_} using {grid_result.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''KNN'''\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred_knn = model.predict(X_test_scaled)\n",
    "new_data_knn = accuracy_score(y_test, y_pred_knn)\n",
    "print(f'accuracy: {new_data_knn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''GreidSearchCV and cross validation for KNN'''\n",
    "model = KNeighborsClassifier()\n",
    "grid = {'n_neighbors': np.arange(1, 20)}\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy', error_score=0)\n",
    "grid_result = grid_search.fit(X_train_scaled, y_train)\n",
    "print(f'Best Score: {grid_result.best_score_} using {grid_result.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Decision Tree'''\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred_dt = model.predict(X_test_scaled)\n",
    "new_data_dt = accuracy_score(y_test, y_pred_dt)\n",
    "print(f'accuracy: {new_data_dt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''GreidSearchCV and cross validation for Decision Tree'''\n",
    "model = DecisionTreeClassifier()\n",
    "grid = {'criterion': ['gini', 'entropy'], 'max_depth': np.arange(1, 20)}\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy', error_score=0)\n",
    "grid_result = grid_search.fit(X_train_scaled, y_train)\n",
    "print(f'Best Score: {grid_result.best_score_} using {grid_result.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Naive Bayes'''\n",
    "model = GaussianNB()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred_nb = model.predict(X_test_scaled)\n",
    "new_data_nb = accuracy_score(y_test, y_pred_nb)\n",
    "print(f'accuracy: {new_data_nb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''GreidSearchCV and cross validation for Naive Bayes'''\n",
    "model = GaussianNB()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid={}, n_jobs=-1, cv=cv, scoring='accuracy', error_score=0)\n",
    "grid_result = grid_search.fit(X_train_scaled, y_train)\n",
    "print(f'Best Score: {grid_result.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''GridSearchCV for MySVMGender'''\n",
    "num_iters = [100, 1000, 10000]\n",
    "lr = [0.001, 0.01, 0.1]\n",
    "param_grid = {'num_iters': num_iters, 'lr': lr, 'verbose': [False]}\n",
    "grid = GridSearchCV(MySVM(), param_grid, scoring='accuracy')\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MySVM(num_iters=10000, lr=0.1, verbose=False)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred_my_svm = model.predict(X_test_scaled)\n",
    "new_data_my_svm = accuracy_score(y_test, y_pred_my_svm)\n",
    "print(f'accuracy: {new_data_my_svm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''GridSearchCV for my logistic regression'''\n",
    "num_iters = [100, 1000, 10000]\n",
    "alpha = [0.001, 0.01, 0.1]\n",
    "param_grid = {'n_iter': num_iters, 'alpha': alpha}\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid = GridSearchCV(MyLogisticRegression(), param_grid, scoring='accuracy', cv=cv)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyLogisticRegression(n_iter=1000, alpha=0.001)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred_my_logistic = model.predict(X_test_scaled)\n",
    "new_data_my_logistic = accuracy_score(y_test, y_pred_my_logistic)\n",
    "print(f'accuracy: {new_data_my_logistic}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 12))\n",
    "\n",
    "plt.suptitle(\"Confusion Matrixes\", fontsize=24)\n",
    "plt.subplots_adjust(wspace = 0.6, hspace= 0.4)\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.title(\"SVM\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_svm), cmap=\"Blues\", annot=True, fmt=\"d\")\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.title(\"KNN\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_knn), cmap='Blues', annot=True, fmt=\"d\")\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.title(\"Naive Bayes\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_nb), cmap='Blues', annot=True, fmt=\"d\")\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.title(\"Decision Tree\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_dt), cmap='Blues', annot=True, fmt=\"d\")\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.title(\"Logistic Regression\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_lg), cmap='Blues', annot=True, fmt=\"d\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Confusion Matrixes for MySVMGender and MyLogisticRegression'''\n",
    "plt.figure(figsize=(24, 12))\n",
    "\n",
    "plt.suptitle(\"Confusion Matrixes\", fontsize=24)\n",
    "plt.subplots_adjust(wspace = 0.6, hspace= 0.4)\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.title(\"MySVMGender\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_my_svm), cmap=\"Blues\", annot=True, fmt=\"d\")\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.title(\"MyLogisticRegression\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_my_logistic), cmap='Blues', annot=True, fmt=\"d\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы:\n",
    "- Лучше всего себя показали модели из sklearn: SVM, KNN и Decision Tree\n",
    "- Моя реализация logistic regression показала результат примерно такой же, как и реализация из sklearn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "ddaee25c3d36155007e8d8878de890371d06e49958582f58a3b72c62fc780c83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
